I'm working with an agentic coding boilerplate project that includes authentication, database integration, and AI capabilities. Here's what's already set up:

## Current Agentic Coding Boilerplate Structure
- Authentication: Better Auth with Google OAuth integration
- Database: Drizzle ORM with PostgreSQL setup
- AI Integration: Vercel AI SDK with OpenAI integration
- UI: shadcn/ui components with Tailwind CSS
- Current Routes:
  - `/` - Home page with setup instructions and feature overview
  - `/dashboard` - Protected dashboard page (requires authentication)
  - `/chat` - AI chat interface (requires OpenAI API key)

## Important Context
This is an agentic coding boilerplate/starter template - all existing pages and components are meant to be examples and should be completely replaced to build the actual AI-powered application.

### CRITICAL: You MUST Override All Boilerplate Content
DO NOT keep any boilerplate components, text, or UI elements unless explicitly requested. This includes:
- Remove all placeholder/demo content (setup checklists, welcome messages, boilerplate text)
- Replace the entire navigation structure - don't keep the existing site header or nav items
- Override all page content completely - don't append to existing pages, replace them entirely
- Remove or replace all example components (setup-checklist, starter-prompt-modal, etc.)
- Replace placeholder routes and pages with the actual application functionality

### Required Actions:
1. Start Fresh: Treat existing components as temporary scaffolding to be removed
2. Complete Replacement: Build the new application from scratch using the existing tech stack
3. No Hybrid Approach: Don't try to integrate new features alongside existing boilerplate content
4. Clean Slate: The final application should have NO trace of the original boilerplate UI or content

The only things to preserve are:
- All installed libraries and dependencies (DO NOT uninstall or remove any packages from package.json)
- Authentication system (but customize the UI/flow as needed)
- Database setup and schema (but modify schema as needed for your use case)
- Core configuration files (next.config.ts, tsconfig.json, tailwind.config.ts, etc.)
- Build and development scripts (keep all npm/pnpm scripts in package.json)

## Tech Stack
- Next.js 15 with App Router
- TypeScript
- Tailwind CSS
- Better Auth for authentication
- Drizzle ORM + PostgreSQL
- Vercel AI SDK
- shadcn/ui components
- Lucide React icons

## AI Model Configuration
IMPORTANT: When implementing any AI functionality, always use the `OPENAI_MODEL` environment variable for the model name instead of hardcoding it:

// ✓ Correct - Use environment variable
const model = process.env.OPENAI_MODEL || "gpt-5-mini";
model: openai(model)

// ✗ Incorrect - Don't hardcode model names
model: openai("gpt-5-mini")

## What I Want to Build
I would like to build **TuneScene**: an AI music-video generator. A user uploads a song (audio) and a scripted timeline (lyrics with timecodes and optional scene prompts) and optionally a singer photo. The app outputs a complete music video with synced lyrics, generated scene clips, optional lip-sync performer segments, stitched together with transitions, and final renders in 16:9 (YouTube) and 9:16 (Shorts/TikTok). The system must avoid any paid AI generation until after payment is collected. Before payment, users see a zero-cost preview (Ken Burns panning + karaoke subtitles) rendered locally with FFmpeg.

### Core User Stories
1. As a user, I can sign in with Google and access a personal dashboard of projects.
2. I can create a project, upload an audio file, paste lyrics, and (optionally) upload assets (images/video; one can be marked as "Singer Photo").
3. I can define or edit a Timeline JSON (validated client/server) specifying scenes with t0/t1, mode (ai_video | image_pan | asset_video | stock), prompts, and overlay options.
4. I can preview instantly for free (Ken Burns + karaoke subtitles) without triggering paid AI APIs.
5. After I pay (one-shot or subscription/credits), the app runs the generation pipeline: lyrics alignment, AI clip generation per scene, optional lip-sync segments, subtitles build, stitching, and final exports.
6. I receive downloadable MP4 files for 1080p 16:9 and 9:16, with project history and re-download links.
7. My usage is metered in minutes/credits; expensive steps only run post-payment.

### Non-Goals (MVP)
- No multi-language dubbing
- No frame-level in-app editing
- No real-time multi-user collaboration

## Replace Boilerplate With Actual App
### New Route Map
- `/` Landing: marketing page, demo videos, CTA “Start Free Preview”
- `/auth/*` Customized auth screens (Better Auth)
- `/projects` List user projects (protected)
- `/projects/new` Create project (title, upload audio, paste lyrics)
- `/projects/:id` Editor (3 panes: Timeline JSON, Lyrics grid, Assets manager)
- `/projects/:id/preview` Zero-cost instant preview (Ken Burns + karaoke)
- `/projects/:id/checkout` Payment screen (one-shot or subscription/credits)
- `/projects/:id/generate` Trigger generation (POST only; protected; requires paid access)
- `/projects/:id/progress` Live job statuses (WebSocket events)
- `/projects/:id/renders` Final files and downloads
- `/pricing` Pricing plans
- `/account` Billing, credits, invoices

### Database (Drizzle ORM) – Tables
- `users(id, email, name, avatar_url, plan, credits, created_at, updated_at)`
- `projects(id, user_id, title, status, duration_ms, created_at, updated_at)`
- `assets(id, project_id, type('audio'|'image'|'video'|'subtitle'), url, meta_json, created_at)`
- `timeline(id, project_id unique, json, version, validated_at, created_at, updated_at)`
- `lyrics(id, project_id, line, start_ms, end_ms, words_json, style_json, created_at)`
- `scenes(id, project_id, index, start_ms, end_ms, mode('ai_video'|'image_pan'|'asset_video'|'stock'), prompt, style_json, performer_json, status, clip_url, created_at)`
- `jobs(id, project_id, type, status, error, payload_json, result_json, created_at, updated_at)`
- `renders(id, project_id, format('1080p_16x9'|'1080p_9x16'), url, duration_ms, size_bytes, created_at)`
- `billing_events(id, user_id, project_id?, kind, amount_cents, metadata_json, created_at)`

### Timeline JSON Schema (Zod to validate)
- meta: { fps, target_formats }
- performer (global): { enabled, lip_sync_from, placement, scale }
- lyrics: [{ t0, t1, text }]
- scenes: [{ idx, t0, t1, mode, prompt?, assetRef?, style?, overlay?, performer? }]
- transitions: [{ from, to, type, ms }]

### Generation Pipeline (after payment)
Job graph executed by a worker with Redis queue (BullMQ or equivalent):
1. ALIGN_LYRICS
   - If user timecodes missing → call Whisper or self-host Aeneas → create `lyrics` rows
2. SCENE_PLAN
   - Expand `timeline.scenes` into clip jobs with durations and provider params
3. CLIP_GENERATE (parallel per scene)
   - ai_video → external provider API (Runway/Pika/Kaiber/Kling adapter)
   - image_pan → FFmpeg Ken Burns
   - asset_video → trim/crop user asset
   - stock → future integration
4. PERFORMER_COMPOSITE (optional per scene)
   - singer photo + audio slice → HeyGen/D-ID or self-host Wav2Lip → composite on scene
5. SUBTITLES_BUILD
   - Build ASS/SRT from lyrics (karaoke effect if words present)
6. STITCH_AND_RENDER
   - Concatenate, add transitions, overlay subs, export 1080p 16:9 master and smart-crop 9:16
7. DELIVER
   - Upload to R2, write `renders`, emit progress events

### Zero-Cost Preview (before payment)
- FFmpeg waveform + karaoke subtitles + Ken Burns on user image or placeholder stock
- No external paid API calls
- Clear callout that full AI scenes and lip-sync unlock after payment

### Provider Adapters (server)
- Alignment: Whisper API (cheap) or self-host Aeneas
- AI Video: Runway, Pika, Kaiber, Kling (pluggable adapters)
- Lip-Sync: HeyGen, D-ID, or self-host Wav2Lip
- Storage: Cloudflare R2 (preferred), signed URLs
- Payments: Stripe (one-shot, subscriptions, credits)

### Environment Variables (.env)
- AUTH_SECRET, GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET
- DATABASE_URL (Postgres)
- OPENAI_API_KEY, OPENAI_MODEL
- STRIPE_SECRET_KEY, STRIPE_WEBHOOK_SECRET
- R2_ACCOUNT_ID, R2_ACCESS_KEY_ID, R2_SECRET_ACCESS_KEY, R2_BUCKET
- RUNWAY_API_KEY, PIKA_API_KEY, KAIBER_API_KEY, KLING_API_KEY
- HEYGEN_API_KEY, DID_API_KEY
- REDIS_URL
- FFMPEG_PATH (if needed)

### Pricing & Guardrails
- One-Shot: pay before render, price per output minute; optional add-on for lip-sync
- Subscription: monthly/annual includes N minutes of AI video; overage uses credits
- Absolutely no AI API calls before payment
- Caps: max duration per project, max scenes, daily cost ceiling per user
- Watermark only after payment failure or for trial assets (not for AI-generated video runs)

### Cost Model Targets (internal)
- Alignment: ~$0.006/min via Whisper or near zero self-host
- AI Scene: $5–$10/min depending on provider/model/quality
- Lip-Sync: $3–$6/min via API; near zero with self-host Wav2Lip (GPU batch)
- FFmpeg compose/export: negligible
- Storage + CDN egress (R2): negligible per video

### Worker/Queue
- Redis queue with per-project job graph
- Idempotent steps, resumable on crash
- Progress events via WebSocket: queued → aligning → generating (n/N) → composing → rendering → delivered
- Structured logs, error redaction (no secrets)

### Frontend (shadcn/ui first)
- New nav: Projects, Pricing, Account
- Project Editor split view:
  - Timeline JSON (Monaco + Zod validation)
  - Lyrics grid (editable times, import/export .lrc/.srt)
  - Assets manager (mark Singer Photo)
- Preview player (free preview)
- Checkout (Stripe) with one-shot and subscription tabs
- Progress board (cards per scene with live statuses)
- Renders page with downloads (16:9 and 9:16)

## Request
Please help me transform this boilerplate into my actual application. You MUST completely replace all existing boilerplate code to match my project requirements. The current implementation is just temporary scaffolding that should be entirely removed and replaced.

## Final Reminder: COMPLETE REPLACEMENT REQUIRED
⚠️ IMPORTANT: Do not preserve any of the existing boilerplate UI, components, or content. The user expects a completely fresh application that implements their requirements from scratch. Any remnants of the original boilerplate (like setup checklists, welcome screens, demo content, or placeholder navigation) indicate incomplete implementation.

Success Criteria: The final application should look and function as if it was built from scratch for TuneScene, with no evidence of the original boilerplate template. It must not trigger paid AI APIs before payment and must provide a zero-cost preview using FFmpeg only.

## Post-Implementation Documentation
After completing the implementation, you MUST document any new features or significant changes in the `/docs/features/` directory:

1. Create Feature Documentation: For each major feature, create a markdown file in `/docs/features/` explaining:
   - What the feature does
   - How it works
   - Key components and files involved
   - Usage examples
   - Any configuration or setup required

2. Update Existing Documentation: If you modify existing functionality, update the relevant documentation files.

3. Document Design Decisions: Include any important architectural or design decisions made during implementation.

Think hard about the solution and implementing the user's requirements.
